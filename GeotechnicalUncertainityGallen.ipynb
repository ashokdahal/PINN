{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import GallenModel as ClassificationModelsimple\n",
    "import geopandas as gpd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  df = dataframe.copy()\n",
    "  labels = df.pop('Landslide')\n",
    "  df = {key: value.to_numpy()[:,tf.newaxis] for key, value in dataframe.items()}\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a layer that turns strings into integer indices.\n",
    "  if dtype == 'string':\n",
    "    index = layers.StringLookup(max_tokens=max_tokens)\n",
    "  # Otherwise, create a layer that turns integer values into integer indices.\n",
    "  else:\n",
    "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Encode the integer indices.\n",
    "  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "  # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "  return lambda feature: encoder(index(feature))\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for the feature.\n",
    "  normalizer = layers.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainmodel(model,train_ds,val_ds,NUMBER_EPOCHS = 100):\n",
    "    \n",
    "    \n",
    "    filepath='TrainedWeightsCrossVal'\n",
    "    BATCH_SIZE=32\n",
    "    \n",
    "    model_checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor=\"val_auc\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode=\"max\",\n",
    "        save_freq=\"epoch\",\n",
    "        options=None\n",
    "    )\n",
    "    print(type(train_ds))\n",
    "    hist = model.fit(train_ds,\n",
    "                     epochs=NUMBER_EPOCHS,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     validation_data=val_ds,\n",
    "                    #  validation_split=0.2,#auto validate using 20% of random samples at each epoch\n",
    "                     verbose=1, callbacks=[model_checkpoint_callback],class_weight = {0: 1, 1: 5}\n",
    "\n",
    "                    )\n",
    "    del model\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BootStrapGeotech(dfc):\n",
    "  n_bootstrap=50\n",
    "  i=0\n",
    "  for i in range(n_bootstrap):\n",
    "    print(i)\n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "    \n",
    "    #    df.iloc[train_index]\n",
    "    # df.iloc[test_index]\n",
    "    train_df= resample(dfc, random_state=None,n_samples=10000, replace=False)\n",
    "    test_df=dfc[~dfc.cat.isin(train_df.cat)]\n",
    "    print(f\"Number of train set{len(train_df)} and number of test set {len(test_df)}\")\n",
    "\n",
    "    exai_ds=df_to_dataset(train_df[['Est_m','Nrt_m','HC_m','VC_m','Slp_m','Prc_m','NDVI_m','PGA_Usgs','Sand_m','Silt_m','Clay_m','Bdod_m','GLG','Landslide']])\n",
    "    val_ds=df_to_dataset(test_df[['Est_m','Nrt_m','HC_m','VC_m','Slp_m','Prc_m','NDVI_m','PGA_Usgs','Sand_m','Silt_m','Clay_m','Bdod_m','GLG','Landslide']],shuffle=False)\n",
    "    y_test=test_df['Landslide'].to_numpy()\n",
    "    \n",
    "\n",
    "    for header in numerical_cols:\n",
    "      numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "      normalization_layer = get_normalization_layer(header, exai_ds)\n",
    "      encoded_numeric_col = normalization_layer(numeric_col)\n",
    "      all_inputs.append(numeric_col)\n",
    "      encoded_features.append(encoded_numeric_col)\n",
    "\n",
    "    for header in categorical_cols:\n",
    "      categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "      encoding_layer = get_category_encoding_layer(name=header,\n",
    "                                                  dataset=exai_ds,\n",
    "                                                  dtype='string',\n",
    "                                                  max_tokens=9)\n",
    "      encoded_categorical_col = encoding_layer(categorical_col)\n",
    "      all_inputs.append(categorical_col)\n",
    "      encoded_features.append(encoded_categorical_col)\n",
    "\n",
    "    clfmdl=ClassificationModelsimple.LandslideModel()\n",
    "    clfmdl.getclassificationModel(all_inputs=all_inputs, encoded_features=encoded_features)\n",
    "    clfmdl.getOptimizer()\n",
    "    clfmdl.compileModel()\n",
    "\n",
    "    trainmodel(clfmdl.model,exai_ds,val_ds,NUMBER_EPOCHS = 50)\n",
    "    del clfmdl.model,clfmdl\n",
    "\n",
    "    model =  tf.keras.models.load_model(\"TrainedWeightsCrossVal/\")\n",
    "    \n",
    "    all_data=df_to_dataset(df[['Est_m','Nrt_m','HC_m','VC_m','Slp_m','Prc_m','NDVI_m','PGA_Usgs','Sand_m','Silt_m','Clay_m','Bdod_m','GLG','Landslide']],shuffle=False)\n",
    "    geo_Tech1 = tf.keras.Model(inputs=model.input, outputs=model.get_layer(\"cohesion\").output)*5 # apply linear scaling to compensate for 5x scaling on loss function\n",
    "    geotech_preds=geo_Tech1.predict(all_data)\n",
    "    np.save(f'GeotechResultsGallen/CoGeotech_{str(i)}.npy',geotech_preds)\n",
    "\n",
    "    geo_Tech2 = tf.keras.Model(inputs=model.input, outputs=model.get_layer(\"internalFriction\").output)\n",
    "    geotech_preds=geo_Tech2.predict(all_data)\n",
    "    np.save(f'GeotechResultsGallen/IfGeotech_{str(i)}.npy',geotech_preds)\n",
    "    \n",
    "    del geo_Tech2\n",
    "    del geo_Tech1\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    i+=1\n",
    "    # del clfmdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['GLG']\n",
    "numerical_cols=['Est_m', 'Nrt_m', 'HC_m', 'VC_m', 'Slp_m', 'Prc_m', 'NDVI_m', 'PGA_Usgs', 'Sand_m', 'Silt_m', 'Clay_m', 'Bdod_m']\n",
    "df=gpd.read_file('Data/NepalEqUSGSGallen.gpkg')\n",
    "BootStrapGeotech(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geotechnical uncertainity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np \n",
    "import geopandas as gpd\n",
    "rcl_indexes=[]\n",
    "df=gpd.read_file('Data/NepalEqUSGSGallen.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "rv=np.load(f'GeotechResultsGallen/CoGeotech_{str(0)}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "co_all = np.empty((50,rv.shape[0]))\n",
    "for i in range(50):\n",
    "    rv=np.load(f'GeotechResultsGallen/CoGeotech_{str(i)}.npy')\n",
    "    co_all[i]=rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if_all = np.empty((50,rv.shape[0]))\n",
    "for i in range(1,11):\n",
    "    rv=np.load(f'GeotechResultsGallen/IfGeotech_{str(i)}.npy')\n",
    "    if_all[i]=rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import contextily as cx\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "\n",
    "fig,axs = plt.subplots(2,2,dpi=500, figsize=(10,10))\n",
    "df_wm = df.to_crs(epsg=3857)\n",
    "co_all[co_all==0.0] = np.nan\n",
    "co_all_bak = co_all\n",
    "df_wm['cohesion'] =np.nanmedian(co_all_bak, axis=0)\n",
    "df_wm['cohesion_95'] = (np.nanquantile(co_all_bak,0.95, axis=0))\n",
    "df_wm['cohesion_5'] = (np.nanquantile(co_all_bak,0.05, axis=0))\n",
    "df_wm = df_wm[df_wm['Slp_m']>10]\n",
    "\n",
    "df_wm.plot(column='cohesion_5',cmap='viridis', legend=True, alpha=0.7, vmin=0.0, vmax=20, ax=axs[0][0],legend_kwds={\"label\": \"cohesion/thickness (KPa/m)\", \"orientation\": \"horizontal\"},)\n",
    "cx.add_basemap(axs[0][0],source='NASAGIBS.ASTER_GDEM_Greyscale_Shaded_Relief')\n",
    "axs[0][0].set_axis_off()\n",
    "axs[0][0].add_artist(ScaleBar(1))\n",
    "\n",
    "df_wm.plot(column='cohesion',cmap='viridis',legend=True, alpha=0.7, vmin=0.0, vmax=20, ax=axs[0][1],legend_kwds={\"label\": \"cohesion/thickness (KPa/m)\", \"orientation\": \"horizontal\"},)\n",
    "cx.add_basemap(axs[0][1],source='NASAGIBS.ASTER_GDEM_Greyscale_Shaded_Relief')\n",
    "axs[0][1].set_axis_off()\n",
    "\n",
    "df_wm.plot(column='cohesion_95',cmap='viridis',legend=True, alpha=0.7, vmin=0.0, vmax=20, ax=axs[1][0],legend_kwds={\"label\": \"cohesion/thickness (KPa/m)\", \"orientation\": \"horizontal\"},)\n",
    "cx.add_basemap(axs[1][0],source='NASAGIBS.ASTER_GDEM_Greyscale_Shaded_Relief')\n",
    "axs[1][0].set_axis_off()\n",
    "\n",
    "axs[1][1].boxplot(co_all_bak[~np.isnan(co_all_bak)],0, '')\n",
    "# axs[1][1].set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"PINNPlotsGallen/Cohesionv5.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import contextily as cx\n",
    "df_wm = df.to_crs(epsg=3857)\n",
    "if_all_d = np.rad2deg(if_all)\n",
    "if_all_d[if_all_d==0.0] = np.nan\n",
    "df_wm['internalfriction'] = np.nanmedian(if_all_d,axis=0)\n",
    "df_wm['internalfriction_95'] = np.nanquantile(if_all_d,0.95,axis=0)\n",
    "df_wm['internalfriction_5'] = np.nanquantile(if_all_d,0.05,axis=0)\n",
    "\n",
    "df_wm = df_wm[df_wm['Slp_m']>10]\n",
    "\n",
    "fig,axs = plt.subplots(2,2,dpi=500, figsize=(15,15))\n",
    "df_wm.plot(column='internalfriction_5',cmap='magma', legend=True, alpha=0.7, vmin=20, vmax=50, ax=axs[0][0],legend_kwds={\"label\": \"Internal Friction Angle ($^\\circ$)\", \"orientation\": \"horizontal\"},)\n",
    "cx.add_basemap(axs[0][0],source='NASAGIBS.ASTER_GDEM_Greyscale_Shaded_Relief')\n",
    "axs[0][0].set_axis_off()\n",
    "axs[0][0].add_artist(ScaleBar(1))\n",
    "\n",
    "df_wm.plot(column='internalfriction',cmap='magma',legend=True, alpha=0.7, vmin=20, vmax=50, ax=axs[0][1],legend_kwds={\"label\": \"Internal Friction Angle ($^\\circ$)\", \"orientation\": \"horizontal\"},)\n",
    "cx.add_basemap(axs[0][1],source='NASAGIBS.ASTER_GDEM_Greyscale_Shaded_Relief')\n",
    "axs[0][1].set_axis_off()\n",
    "\n",
    "df_wm.plot(column='internalfriction_95',cmap='magma',legend=True, alpha=0.7, vmin=20, vmax=50, ax=axs[1][0],legend_kwds={\"label\": \"Internal Friction Angle ($^\\circ$)\", \"orientation\": \"horizontal\"},)\n",
    "cx.add_basemap(axs[1][0],source='NASAGIBS.ASTER_GDEM_Greyscale_Shaded_Relief')\n",
    "axs[1][0].set_axis_off()\n",
    "\n",
    "axs[1][1].boxplot(if_all_d[~np.isnan(if_all_d)],0, '')\n",
    "\n",
    "plt.savefig(\"PINNPlotsGallen/Frictionv5.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
